# GPT-2 from Scratch

Welcome to the `gpt2-from-scratch` repository! This project implements the pre-training and fine-tuning of the GPT-2 model from scratch. The repository includes:

- **Pre-training**: Training the GPT-2 model on a large corpus of text data.
- **Fine-tuning for Classification**: Fine-tuning the pre-trained GPT-2 model for specific classification tasks.
- **LoRA Fine-tuning**: (Coming soon) Fine-tuning using Low-Rank Adaptation (LoRA) techniques.
- **Instruction Fine-tuning**: (Coming soon) Fine-tuning the GPT-2 model for following instructions more effectively.

## Table of Contents

- [Installation](#installation)
- [Usage](#usage)
  - [Pre-training](#pre-training)
  - [Fine-tuning for Classification](#fine-tuning-for-classification)
- [Future Work](#future-work)
- [Contributing](#contributing)
- [License](#license)

## Installation

To get started, clone the repository and install the necessary dependencies:

```bash
git clone https://github.com/ardaaras99/gpt2-from-scratch.git
cd gpt2-from-scratch
pip install -r requirements.txt
