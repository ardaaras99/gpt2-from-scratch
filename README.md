# GPT-2 from Scratch

Welcome to the `gpt2-from-scratch` repository! This project implements the pre-training and fine-tuning of the GPT-2 model from scratch. The repository includes:

- **Pre-training**: Training the GPT-2 model on a large corpus of text data.
- **Fine-tuning for Classification**: Fine-tuning the pre-trained GPT-2 model for specific classification tasks.
- **LoRA Fine-tuning**: (Coming soon) Fine-tuning using Low-Rank Adaptation (LoRA) techniques.
- **Instruction Fine-tuning**: (Coming soon) Fine-tuning the GPT-2 model for following instructions more effectively.
- **Loading GPT-2 Pre-Trained weights**: (Comming soon) With this future you will be able to load the original weights of GPT-2 (in Tensorflow) to your PyTorch model.

## Installation

To get started, clone the repository and install the necessary dependencies:

```bash
git clone https://github.com/ardaaras99/gpt2-from-scratch.git
cd gpt2-from-scratch
poetry install gpt2-from-scratch
